{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative rotation estimation and Rotation averaging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/ignaciozm/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "#importing the opencv module  \n",
    "import cv2  \n",
    "\n",
    "path = '../data/img/dog.jpg'\n",
    "# using imread('path') and 1 denotes read as  color image  \n",
    "img = cv2.imread(path,1)  \n",
    "#This is using for display the image  \n",
    "cv2.imshow('image',img)  \n",
    "cv2.waitKey() # This is necessary to be required so that the image doesn't close immediately.  \n",
    "#It will run continuously until the key press.  \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.70710678   0.70710678 -24.42745289]\n",
      " [ -0.70710678   0.70710678 124.02691193]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#importing the opencv module  \n",
    "import cv2  \n",
    "# using imread('path') and 1 denotes read as  color image  \n",
    "img = cv2.imread(path,0)  \n",
    "# get image height, width\n",
    "(h, w) = img.shape[:2]\n",
    "# calculate the center of the image\n",
    "center = (w / 2, h / 2)\n",
    " \n",
    "scale = 1.0\n",
    " \n",
    "# Perform the counter clockwise rotation holding at the center\n",
    "# 45 degrees\n",
    "M = cv2.getRotationMatrix2D(center, 45, scale)\n",
    "print(M)\n",
    "rotated45 = cv2.warpAffine(img, M, (h, w))\n",
    " \n",
    "# 110 degrees\n",
    "M = cv2.getRotationMatrix2D(center,180, scale)\n",
    "rotated180 = cv2.warpAffine(img, M, (w, h))\n",
    " \n",
    "# 150 degrees\n",
    "M = cv2.getRotationMatrix2D(center, 150, scale)\n",
    "rotated150 = cv2.warpAffine(img, M, (h, w))\n",
    " \n",
    " \n",
    "cv2.imshow('Original Image',img)\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image\n",
    " \n",
    "cv2.imshow('Image rotated by 45 degrees',rotated45)\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image\n",
    " \n",
    "cv2.imshow('Image rotated by 110 degrees',rotated180)\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image\n",
    " \n",
    "cv2.imshow('Image rotated by 150 degrees',rotated150)\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy/OpenCV implementation\n",
    "import cv2\n",
    "from scipy import signal\n",
    "import scipy.fft as sfft\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# OpenCV implementation\n",
    "def find_rotation_cv(img_ref: np.ndarray, img_moved: np.ndarray) -> float:\n",
    "    h, w = img_ref.shape[:2]\n",
    "    hamming_w = signal.windows.hamming(w)\n",
    "    hamming_h = signal.windows.hamming(h)\n",
    "    hamming = np.outer(hamming_h, hamming_w)\n",
    "\n",
    "    F_ref = np.log(np.abs(sfft.fftshift(sfft.fft2(img_ref * hamming))))\n",
    "    F_moved = np.log(np.abs(sfft.fftshift(sfft.fft2(img_moved * hamming))))\n",
    "\n",
    "    center_x = w // 2\n",
    "    center_y = h // 2\n",
    "    radius = min(center_x, center_y)\n",
    "\n",
    "    # Define the desired size of the output polar image\n",
    "    polar_width = radius\n",
    "    polar_height = int(np.ceil(radius * np.pi / 2))\n",
    "\n",
    "    # Perform the polar transformation\n",
    "    F_ref_warpped = cv2.warpPolar(\n",
    "        F_ref, \n",
    "        (polar_width, polar_height), \n",
    "        (center_x, center_y), \n",
    "        radius, \n",
    "        cv2.WARP_POLAR_LOG + cv2.INTER_CUBIC + cv2.WARP_FILL_OUTLIERS\n",
    "    )\n",
    "    F_moved_warpped = cv2.warpPolar(\n",
    "        F_moved, \n",
    "        (polar_width, polar_height), \n",
    "        (center_x, center_y), \n",
    "        radius, \n",
    "        cv2.WARP_POLAR_LOG + cv2.INTER_CUBIC + cv2.WARP_FILL_OUTLIERS\n",
    "    )\n",
    "\n",
    "    ret = cv2.phaseCorrelate(F_ref_warpped[:180], F_moved_warpped[:180])\n",
    "    theta_shift = 360 / polar_height * -ret[0][1]\n",
    "    return theta_shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of rotated180 (183, 275) shape of img (183, 275)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04617900679547277"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of rotated180', rotated180.shape, 'shape of img', img.shape), \n",
    "find_rotation_cv(img, rotated180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m     img1Reg \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(img1, h, (width, height))\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img1Reg, h\n\u001b[0;32m---> 50\u001b[0m \u001b[43mpredict_rotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotated180\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotated180\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m, in \u001b[0;36mpredict_rotation\u001b[0;34m(img1, img2)\u001b[0m\n\u001b[1;32m     19\u001b[0m matches \u001b[38;5;241m=\u001b[39m matcher\u001b[38;5;241m.\u001b[39mmatch(descriptors1, descriptors2, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Sort matches by score\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdistance, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Remove not so good matches\u001b[39;00m\n\u001b[1;32m     25\u001b[0m numGoodMatches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matches) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.15\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "def predict_rotation(img1, img2):\n",
    "    # Predicts the rotation between two images\n",
    "    # img1: first image\n",
    "    # img2: second image\n",
    "    # returns: predicted rotation in degrees\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    if len(img1.shape) > 2:\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    # Match features.\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * 0.15)\n",
    "    matches = matches[:numGoodMatches]\n",
    "\n",
    "    # Draw top matches\n",
    "    imgMatches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches, None)\n",
    "    cv2.imshow(\"Matches\", imgMatches)\n",
    "    cv2.waitKey()\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # Use homography\n",
    "    height, width, channels = img2.shape\n",
    "    img1Reg = cv2.warpPerspective(img1, h, (width, height))\n",
    "\n",
    "    return img1Reg, h\n",
    "\n",
    "predict_rotation(rotated180, rotated180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m         angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marccos((np\u001b[38;5;241m.\u001b[39mtrace(R) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m angle\n\u001b[0;32m---> 46\u001b[0m \u001b[43mEstimateRot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rotation_angle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotated180\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotated180\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 41\u001b[0m, in \u001b[0;36mEstimateRot.get_rotation_angle\u001b[0;34m(self, predicted, real)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rotation_angle\u001b[39m(\u001b[38;5;28mself\u001b[39m, predicted, real):\n\u001b[0;32m---> 41\u001b[0m     R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marccos((np\u001b[38;5;241m.\u001b[39mtrace(R) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m angle\n",
      "Cell \u001b[0;32mIn[27], line 34\u001b[0m, in \u001b[0;36mEstimateRot.get_rotation\u001b[0;34m(self, predicted, real)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rotation\u001b[39m(\u001b[38;5;28mself\u001b[39m, predicted, real):\n\u001b[0;32m---> 34\u001b[0m     tform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_rotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     w, u, vt \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mSVDecomp(tform)\n\u001b[1;32m     36\u001b[0m     u, v \u001b[38;5;241m=\u001b[39m vt\u001b[38;5;241m.\u001b[39mT, u\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m, in \u001b[0;36mEstimateRot.predict_rotation\u001b[0;34m(self, predicted, real)\u001b[0m\n\u001b[1;32m      8\u001b[0m p2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predicted)\n\u001b[1;32m      9\u001b[0m p1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(real)\n\u001b[0;32m---> 10\u001b[0m p1dbl \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m p2dbl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((p2, np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mlen\u001b[39m(p2), \u001b[38;5;241m1\u001b[39m))))\n\u001b[1;32m     12\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(p2dbl)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "\n",
    "class EstimateRot:\n",
    "    def __init__(self):\n",
    "        self.K = np.array([[246.43059488719612, 0, 103.08424033768227],\n",
    "                           [0, 245.6414223852361, 77.55187058195787],\n",
    "                           [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    def predict_rotation(self, predicted, real):\n",
    "        p2 = np.array(predicted)\n",
    "        p1 = np.array(real)\n",
    "        p1dbl = np.hstack((p1, np.ones((len(p1), 1))))\n",
    "        p2dbl = np.hstack((p2, np.ones((len(p2), 1))))\n",
    "        prediction = np.float32(p2dbl)\n",
    "        reality = np.float32(p1dbl)\n",
    "        A = np.zeros((3, 3), dtype=np.float32)\n",
    "        tform = np.eye(3, dtype=np.float32)\n",
    "        reality = cv2.gemm(cv2.invert(self.K)[1], reality.T, 1, tform, 0)\n",
    "        prediction = cv2.gemm(cv2.invert(self.K)[1], prediction.T, 1, tform, 0)\n",
    "        A = cv2.gemm(reality, prediction.T, 1, tform, 0)\n",
    "        w, u, vt = cv2.SVDecomp(A)\n",
    "        u, v = vt.T, u.T\n",
    "        A = cv2.gemm(v, u, 1, tform, 0)\n",
    "        det = np.linalg.det(A)\n",
    "        if det < 0:\n",
    "            v[:, 2] *= -1\n",
    "            A = cv2.gemm(v, u, 1, tform, 0)\n",
    "            det = np.linalg.det(A)\n",
    "        tform[2, 2] = det\n",
    "        temp2 = A\n",
    "        temp2 = cv2.gemm(v, tform, 1, tform, 0)\n",
    "        tform = cv2.gemm(temp2, u, 1, tform, 0)\n",
    "        return tform\n",
    "    \n",
    "    def get_rotation(self, predicted, real):\n",
    "        tform = self.predict_rotation(predicted, real)\n",
    "        w, u, vt = cv2.SVDecomp(tform)\n",
    "        u, v = vt.T, u.T\n",
    "        R = cv2.gemm(v, u, 1, tform, 0)\n",
    "        return R\n",
    "    \n",
    "    def get_rotation_angle(self, predicted, real):\n",
    "        R = self.get_rotation(predicted, real)\n",
    "        angle = np.arccos((np.trace(R) - 1) / 2)\n",
    "        return angle\n",
    "\n",
    "\n",
    "EstimateRot().get_rotation_angle(rotated180, rotated180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
